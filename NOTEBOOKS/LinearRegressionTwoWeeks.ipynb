{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f915cf7-1e1c-479b-95e0-363c276b129a",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION FOR THE NEXT 2 WEEKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9d227b-665b-412b-b3f4-ba9f1891111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ada51a-d1d2-4b4a-bbc2-cbe566284731",
   "metadata": {},
   "source": [
    "### 1. Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4666e0-75a1-4a4a-abbe-26c74fbb00ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66bd9e5-053b-4bd7-93ee-d688ef14aa87",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Window\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assume 'song_id' is the unique identifier for the track/artist combination\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# and 'Seconds since Epoch' provides the chronological order.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msong_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeconds since Epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_week_rank\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     F\u001b[38;5;241m.\u001b[39mlead(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_rank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mover(window_spec)\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_week_ahead_rank\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     F\u001b[38;5;241m.\u001b[39mlead(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_rank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mover(window_spec)\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:222\u001b[0m, in \u001b[0;36mtry_remote_window.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(Window, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\window.py:128\u001b[0m, in \u001b[0;36mWindow.partitionBy\u001b[1;34m(*cols)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@try_remote_window\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartitionBy\u001b[39m(\u001b[38;5;241m*\u001b[39mcols: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m, List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName_\u001b[39m\u001b[38;5;124m\"\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindowSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Creates a :class:`WindowSpec` with the partitioning defined.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    +---+--------+----------+\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[0;32m    129\u001b[0m     jspec \u001b[38;5;241m=\u001b[39m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexpressions\u001b[38;5;241m.\u001b[39mWindow\u001b[38;5;241m.\u001b[39mpartitionBy(\n\u001b[0;32m    130\u001b[0m         _to_java_cols(cols)\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WindowSpec(jspec)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:248\u001b[0m, in \u001b[0;36mget_active_spark_context\u001b[1;34m()\u001b[0m\n\u001b[0;32m    246\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext or SparkSession should be created first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[1;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assume 'song_id' is the unique identifier for the track/artist combination\n",
    "# and 'Seconds since Epoch' provides the chronological order.\n",
    "window_spec = Window.partitionBy('song_id').orderBy('Seconds since Epoch')\n",
    "\n",
    "df = df.withColumn(\n",
    "    'next_week_rank',\n",
    "    F.lead('peak_rank', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    'two_week_ahead_rank',\n",
    "    F.lead('peak_rank', 2).over(window_spec)\n",
    ")\n",
    "\n",
    "# Drop rows where the target is null (the last two weeks of data for each song)\n",
    "df_train = df.na.drop(subset=['next_week_rank', 'two_week_ahead_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c386c36-becd-46d4-9704-50d9ee3f18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List the features you want to use\n",
    "feature_columns = [\n",
    "    'trackAppearanceCount', 'artistAppearanceCount', 'peak_rank',\n",
    "    'previous_rank', 'weeks_on_chart', 'streams', 'rank difference',\n",
    "    'Position over Time', 'ArtistCount', 'isTopTen', 'IsGirlGroup',\n",
    "    'IsBoyGroup', 'IsMixedGroup'\n",
    "]\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa0604-4ca7-41bd-80f1-6a5a7bcedc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is ordered by time, select a cutoff point\n",
    "# For simplicity, let's use a random split for now, but in time-series\n",
    "# use a time-based split (e.g., 90% oldest for training, 10% newest for testing)\n",
    "train_df, test_df = df_train.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d02fd-5d4f-4191-b779-2e693d934990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# 1. Initialize the Linear Regression estimator\n",
    "lr_t1 = LinearRegression(\n",
    "    labelCol='next_week_rank', # Target variable\n",
    "    featuresCol='features',\n",
    "    regParam=0.1,             # Regularization parameter\n",
    "    elasticNetParam=0.0       # 0.0 for L2 (Ridge) regularization\n",
    ")\n",
    "\n",
    "# 2. Create the Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline_t1 = Pipeline(stages=[vector_assembler, lr_t1])\n",
    "\n",
    "# 3. Train the model\n",
    "model_t1 = pipeline_t1.fit(train_df)\n",
    "\n",
    "print(\"T+1 Model Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bd7dc-5c46-472d-9d82-7c6a6d65af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Linear Regression estimator\n",
    "lr_t2 = LinearRegression(\n",
    "    labelCol='two_week_ahead_rank', # New target variable\n",
    "    featuresCol='features',\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "# 2. Create the Pipeline\n",
    "pipeline_t2 = Pipeline(stages=[vector_assembler, lr_t2])\n",
    "\n",
    "# 3. Train the model\n",
    "model_t2 = pipeline_t2.fit(train_df)\n",
    "\n",
    "print(\"T+2 Model Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f29050-b0ef-453e-8892-6388ec8c4d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75519be3-4a31-48e5-940d-8412ea8d75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original rows where the targets were NULL (the latest two weeks of data per song)\n",
    "df_pred = df.filter(F.col('next_week_rank').isNull() | F.col('two_week_ahead_rank').isNull())\n",
    "\n",
    "# Further filtering might be necessary to isolate ONLY the latest week (t) and second-to-latest week (t-1)\n",
    "# to predict t+1 and t+2, respectively.\n",
    "# For a robust prediction, you usually use the very last week (t) of known data for each song.\n",
    "\n",
    "# Get only the last known observation (time t) for each song to predict t+1 and t+2\n",
    "window_last = Window.partitionBy('song_id').orderBy(F.desc('Seconds since Epoch'))\n",
    "df_latest = df.withColumn('row_num', F.row_number().over(window_last)).filter(F.col('row_num') == 1).drop('row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9149e-600e-44b6-96d1-8de943114769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093cc434-17d5-482f-a89e-3de2d2e6aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict rank for the next week (t+1)\n",
    "predictions_t1 = model_t1.transform(df_latest).select(\n",
    "    'song_id', 'peak_rank', 'Weeks_on_chart',\n",
    "    F.round(F.col('prediction')).alias('predicted_rank_t_plus_1')\n",
    ")\n",
    "\n",
    "# Predict rank for the week after (t+2)\n",
    "predictions_t2 = model_t2.transform(df_latest).select(\n",
    "    'song_id',\n",
    "    F.round(F.col('prediction')).alias('predicted_rank_t_plus_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bb1e5-24e8-4ba0-b92c-9a07e6e94f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6234fd-7898-4b51-b917-aa9927446aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = predictions_t1.join(predictions_t2, on='song_id', how='inner')\n",
    "\n",
    "final_predictions.show()\n",
    "# Example output:\n",
    "# +-------+---------+----------------+-----------------------+-----------------------+\n",
    "# |song_id|peak_rank|Weeks_on_chart|predicted_rank_t_plus_1|predicted_rank_t_plus_2|\n",
    "# +-------+---------+----------------+-----------------------+-----------------------+\n",
    "# |song_A |    5    |      10        |           7           |           10          |\n",
    "# |song_B |   45    |       3        |          40           |          35           |\n",
    "# +-------+---------+----------------+-----------------------+-----------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eeaa9-e9b1-4bc5-8a32-3e474a273ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f329e-48cb-4ce5-9317-321334dbe1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759d989-eb83-40fc-849e-9d8e24435ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
